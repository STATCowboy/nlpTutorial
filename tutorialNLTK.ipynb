{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "# nltk.download()#If needed download required nltk text\n",
    "\n",
    "'''\n",
    "NLTK is an incredibly useful python package for doing many basic text manipulations.\n",
    "Like many packages, NLTK has its own object that enables much of its functionality.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First, create a NLTK text object by importing a text file or importing from NLTK corpus\n",
    "'''\n",
    "def createNLTKObject(txtFile):\n",
    "    f=open(txtFile,'rU')\n",
    "    raw=f.read()\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    text = nltk.Text(tokens)\n",
    "    return text\n",
    "from nltk.book import text3 #imports The Book of Genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To get the context of a word or a dispersion plot\n",
    "'''\n",
    "text3.concordance(\"God\") #this shows every occurrence with some context\n",
    "text3.similar(\"God\") #shows other words that appear in a similar context\n",
    "text3.dispersion_plot([\"God\",\"Adam\",\"Noah\",\"Abraham\",\"Joseph\"])\n",
    "# print(len(text3))\n",
    "\n",
    "#shows and lists the vocabulary (unique) in text3.  Sorted puts capitalized words first.\n",
    "# vocabSize=sorted(set(text3)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lexical diversity is how many times, on average, a word is used in the entire text\n",
    "'''\n",
    "def lexicalDiversity(text): #this function shows how many times on average a word is used\n",
    "    return len(text)/len(set(text)) #larger results mean less diversity, lower is high diversity\n",
    "\n",
    "# print(lexicalDiversity(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Frequency distributions show the tallies of each word used\n",
    "'''\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist(text3) #this gives the frequency distribution of every word. i.e. tallies of each word used\n",
    "vocab = fdist.items() #list/dict of words from freq dist with keys and values\n",
    "# print(vocab)\n",
    "# hapaxes=fdist.hapaxes() #hapaxes are words that only appear once\n",
    "# fdist.plot(25) #plot 25 most common tokens\n",
    "# fdist.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can try to filter out and only get important words of a critical length and occurrence\n",
    "uniqueWord = set(text3)\n",
    "importantWords = [wd for wd in uniqueWord if len(wd)>5 and fdist[wd]>10] \n",
    "# print(sorted(importantWords))\n",
    "\n",
    "#Collocations - show words that appear together most often\n",
    "c=text3.collocations()\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract num chars, words and sents\n",
    "numChar=0\n",
    "numSents=0\n",
    "for ind in text3:\n",
    "    numChar = numChar+len(ind)\n",
    "    #import os\n",
    "    #os.system(\"say '{}'\".format(ind))\n",
    "    if ind==\".\":\n",
    "        numSents=numSents+1\n",
    "numWords=len(text3)\n",
    "avgWordLen = numChar/numWords\n",
    "wordsPerSent = numWords/numSents\n",
    "# print(\"There are {} total words.  The average word length is {} and there are {} words per sentence on average.\".format(\n",
    "#     numWords,avgWordLen,wordsPerSent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentiment analysis using open source packages: Textblob, Polyglot, and NLTK\n",
    "'''\n",
    "\n",
    "def sentimentTB(sentence):\n",
    "    from textblob.blob import TextBlob\n",
    "    sentBlob = TextBlob(sentence)\n",
    "    sentiment = sentBlob.sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "def sentimentPG(object):\n",
    "    from polyglot.text import Text\n",
    "    text = Text(object)\n",
    "    try:\n",
    "        out = text.polarity\n",
    "    except ZeroDivisionError:\n",
    "        out = 0\n",
    "    return out\n",
    "    \n",
    "def sentimentNL(object):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(object)\n",
    "    return (-1*sentiment['neg'])+sentiment['pos']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
